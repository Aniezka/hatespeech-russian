{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJZkoXegt0lR",
        "outputId": "df8330cd-985a-4a4a-80f1-c002be5637d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.12.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bo5mLcNmtfxD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g07Mvcuttxn",
        "outputId": "f842db3b-e3ba-4c6f-eeb4-8969b40d9dfb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
        "model = AutoModel.from_pretrained('DeepPavlov/rubert-base-cased', output_hidden_states=True).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aJ8BDB7tzyg"
      },
      "outputs": [],
      "source": [
        "def find_cosin_similarity(text_1, text_2, text_3):\n",
        "    tok1 = tokenizer(text_1, return_tensors='pt')\n",
        "    tok2 = tokenizer(text_2, return_tensors='pt')\n",
        "    tok3 = tokenizer(text_3, return_tensors='pt')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out1 = model(tok1.input_ids)\n",
        "        out2 = model(tok2.input_ids)\n",
        "        out3 = model(tok3.input_ids)\n",
        "    \n",
        "    # Only grab the last hidden state\n",
        "    states1 = out1.last_hidden_state.squeeze()\n",
        "    states2 = out2.last_hidden_state.squeeze()\n",
        "    states3 = out3.last_hidden_state.squeeze()\n",
        "    \n",
        "    # average words vectors\n",
        "    avg1 = states1.mean(axis=0)\n",
        "    avg2 = states2.mean(axis=0)\n",
        "    avg3 = states3.mean(axis=0)\n",
        "    \n",
        "    cosin_origin = torch.cosine_similarity(avg1.reshape(1,-1), avg2.reshape(1,-1))\n",
        "    cosin_non_toxic = torch.cosine_similarity(avg1.reshape(1,-1), avg3.reshape(1,-1))\n",
        "    if cosin_origin < cosin_non_toxic: \n",
        "        who_won = 'toxic_original'\n",
        "    else: \n",
        "        who_won = 'non_toxic_ipa'\n",
        "\n",
        "    return cosin_origin, cosin_non_toxic, who_won"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOx_6aCdtt9m"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/dataset_to_model.csv')\n",
        "replica_1 = list(data[\"Реплика 1 toxicity\"])\n",
        "replica_2_toxic_original = list(data[\"Реплика 2 toxicity\"])\n",
        "replica_2_nontoxic_ipa_1 = list(data[\"Реплика 2 original dialogue 1\"])\n",
        "#replica_2_nontoxic_ipa_2 = list(data[\"Реплика 2 original dialogue 2\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viiz3Hh5GaVI",
        "outputId": "25282848-8cf0-4f89-8fb4-27f5b7b66c21"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "13321it [1:56:19,  1.91it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ],
      "source": [
        "cosins_origins_toxic = []\n",
        "cosins_non_toxics_ipa = []\n",
        "who_wins = []\n",
        "for text_1, text_2, text_3 in tqdm(zip(replica_1, replica_2_toxic_original, replica_2_nontoxic_ipa_1)):\n",
        "    cosin_origin, cosin_non_toxic, who_won = find_cosin_similarity(text_1, text_2, text_3)\n",
        "    cosins_origins_toxic.append(cosin_origin)\n",
        "    cosins_non_toxics_ipa.append(cosin_non_toxic)\n",
        "    who_wins.append(who_won)\n",
        "\n",
        "data.cosins_origins = cosins_origins_toxic \n",
        "data.cosins_non_toxics_1 = cosins_non_toxics_ipa\n",
        "data.who_wins_1 = who_wins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QGSAyXhSHNr"
      },
      "outputs": [],
      "source": [
        "data['cosins_non_toxics_1'] = cosins_non_toxics_ipa\n",
        "data['who_wins_1'] = who_wins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPgeE9WeSNIl"
      },
      "outputs": [],
      "source": [
        "data['cosins_origins'] = cosins_origins_toxic "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b74HcxKe04hM"
      },
      "outputs": [],
      "source": [
        "data.to_csv('rubert.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neY4LscRHde5",
        "outputId": "58bc4587-d1cb-4317-abf3-ba58dcaf1a63"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "13321it [1:55:58,  1.91it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ],
      "source": [
        "cosins_origins_toxic = []\n",
        "cosins_non_toxics_ipa = []\n",
        "who_wins = []\n",
        "for text_1, text_2, text_3 in tqdm(zip(replica_1, replica_2_toxic_original, replica_2_nontoxic_ipa_2)):\n",
        "    cosin_origin, cosin_non_toxic, who_won = find_cosin_similarity(text_1, text_2, text_3)\n",
        "    cosins_origins_toxic.append(cosin_origin)\n",
        "    cosins_non_toxics_ipa.append(cosin_non_toxic)\n",
        "    who_wins.append(who_won)\n",
        "\n",
        "data.cosins_non_toxics_2 = cosins_non_toxics_ipa\n",
        "data.who_wins_2 = who_wins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH1u6P-hJe46"
      },
      "outputs": [],
      "source": [
        "data.to_csv('rubert_2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "toxic_original    12406\n",
              "non_toxic_ipa       915\n",
              "Name: who_wins_1, dtype: int64"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('rubert.csv')\n",
        "data['who_wins_1'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Реплика 1 toxicity</th>\n",
              "      <th>Реплика 2 toxicity</th>\n",
              "      <th>Результат 1</th>\n",
              "      <th>Результат 2</th>\n",
              "      <th>Значение 1</th>\n",
              "      <th>Значение 2</th>\n",
              "      <th>Реплика 1 original</th>\n",
              "      <th>Реплика 2 original</th>\n",
              "      <th>Реплика 2 lemma</th>\n",
              "      <th>Выделенные слова</th>\n",
              "      <th>target_group</th>\n",
              "      <th>ipm</th>\n",
              "      <th>zipf</th>\n",
              "      <th>Реплика 2 original dialogue 1</th>\n",
              "      <th>Реплика 2 original dialogue 2</th>\n",
              "      <th>cosins_non_toxics_1</th>\n",
              "      <th>who_wins_1</th>\n",
              "      <th>cosins_origins</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>м когда мужики спокойно за прыгают. пиздомраз...</td>\n",
              "      <td>слышишь блядь заткни свой ебальник сука</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>0.990932</td>\n",
              "      <td>0.997031</td>\n",
              "      <td>150 м Когда мужики спокойно за 250 прыгают. П...</td>\n",
              "      <td>Слышишь блядь заткни свой ебальник сука</td>\n",
              "      <td>['слышать', 'блядь', 'заткнуть', 'свой', 'ебал...</td>\n",
              "      <td>блядь, сука</td>\n",
              "      <td>women</td>\n",
              "      <td>[3642.207530790995, 1712.2843529757424]</td>\n",
              "      <td>[1507.0, 430.57142857142856]</td>\n",
              "      <td>Это ты сейчас о чём?</td>\n",
              "      <td>Вот они и хотят быть равноправными участниками...</td>\n",
              "      <td>tensor([0.4488])</td>\n",
              "      <td>toxic_original</td>\n",
              "      <td>tensor([0.4709])</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>журналюхи развели проф. бабуина на разговор о ...</td>\n",
              "      <td>я высказал предположение что ... но естествен...</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>0.983081</td>\n",
              "      <td>0.994750</td>\n",
              "      <td>журналюхи развели проф. Бабуина на разговор о ...</td>\n",
              "      <td>я высказал предположение что ... но естествен...</td>\n",
              "      <td>['я', 'высказать', 'предположение', 'что', '.....</td>\n",
              "      <td>блядь, сука</td>\n",
              "      <td>women</td>\n",
              "      <td>[3642.207530790995, 1712.2843529757424]</td>\n",
              "      <td>[1507.0, 430.57142857142856]</td>\n",
              "      <td>Такое случается и в жизни 🙁Лучше поменьше чита...</td>\n",
              "      <td>Мне кажется, тут неуместно говорить «педераст»...</td>\n",
              "      <td>tensor([0.4352])</td>\n",
              "      <td>toxic_original</td>\n",
              "      <td>tensor([0.7187])</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>возможности карьеры прям про двачера для двач...</td>\n",
              "      <td>блядь сука пиздец вот повезло ебаный рот за чт...</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0.079426</td>\n",
              "      <td>0.996924</td>\n",
              "      <td>Возможности карьеры 1991 Прям про двачера для...</td>\n",
              "      <td>БЛЯДЬ СУКА ПИЗДЕЦ ВОТ ПОВЕЗЛО ЕБАНЫЙ РОТ ЗА ЧТ...</td>\n",
              "      <td>['блядь', 'сук', 'пиздец', 'вот', 'повезти', '...</td>\n",
              "      <td>блядь, сука</td>\n",
              "      <td>women</td>\n",
              "      <td>[3642.207530790995, 1712.2843529757424]</td>\n",
              "      <td>[1507.0, 430.57142857142856]</td>\n",
              "      <td>А что случилось с твоими возможностями в 1991 ...</td>\n",
              "      <td>А сейчас у тебя какая карьера?</td>\n",
              "      <td>tensor([0.3511])</td>\n",
              "      <td>toxic_original</td>\n",
              "      <td>tensor([0.4075])</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>в византии не было университетов, мань</td>\n",
              "      <td>в византии были государственные школы. до юсти...</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0.001312</td>\n",
              "      <td>0.975552</td>\n",
              "      <td>В Византии не было университетов, мань</td>\n",
              "      <td>В Византии были государственные школы. До Юсти...</td>\n",
              "      <td>['в', 'византия', 'быть', 'государственный', '...</td>\n",
              "      <td>блядь, сука</td>\n",
              "      <td>women</td>\n",
              "      <td>[3642.207530790995, 1712.2843529757424]</td>\n",
              "      <td>[1507.0, 430.57142857142856]</td>\n",
              "      <td>У них были только академии</td>\n",
              "      <td>Как же так? Там были философские школы</td>\n",
              "      <td>tensor([0.4177])</td>\n",
              "      <td>toxic_original</td>\n",
              "      <td>tensor([0.5465])</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>я думал на земле нет таких людей которые смогл...</td>\n",
              "      <td>лол мы чуть - не проебали сука я убил куры уби...</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0.001034</td>\n",
              "      <td>0.995478</td>\n",
              "      <td>я думал на земле нет таких людей которые смогл...</td>\n",
              "      <td>ЛОЛ мы чуть 4-5 не проебали СУКА Я УБИЛ КУРЫ У...</td>\n",
              "      <td>['лола', 'мы', 'чуть', '-', 'не', 'проебали', ...</td>\n",
              "      <td>блядь, сука</td>\n",
              "      <td>women</td>\n",
              "      <td>[3642.207530790995, 1712.2843529757424]</td>\n",
              "      <td>[1507.0, 430.57142857142856]</td>\n",
              "      <td>Согласна, есть много странных и даже опасных в...</td>\n",
              "      <td>Согласна. Я слышала, что даже среди вегетариан...</td>\n",
              "      <td>tensor([0.6395])</td>\n",
              "      <td>non_toxic_ipa</td>\n",
              "      <td>tensor([0.4455])</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                 Реплика 1 toxicity  \\\n",
              "0           0   м когда мужики спокойно за прыгают. пиздомраз...   \n",
              "1           1  журналюхи развели проф. бабуина на разговор о ...   \n",
              "2           2   возможности карьеры прям про двачера для двач...   \n",
              "3           3             в византии не было университетов, мань   \n",
              "4           4  я думал на земле нет таких людей которые смогл...   \n",
              "\n",
              "                                  Реплика 2 toxicity  Результат 1  \\\n",
              "0            слышишь блядь заткни свой ебальник сука         True   \n",
              "1   я высказал предположение что ... но естествен...         True   \n",
              "2  блядь сука пиздец вот повезло ебаный рот за чт...        False   \n",
              "3  в византии были государственные школы. до юсти...        False   \n",
              "4  лол мы чуть - не проебали сука я убил куры уби...        False   \n",
              "\n",
              "   Результат 2  Значение 1  Значение 2  \\\n",
              "0         True    0.990932    0.997031   \n",
              "1         True    0.983081    0.994750   \n",
              "2         True    0.079426    0.996924   \n",
              "3         True    0.001312    0.975552   \n",
              "4         True    0.001034    0.995478   \n",
              "\n",
              "                                  Реплика 1 original  \\\n",
              "0   150 м Когда мужики спокойно за 250 прыгают. П...   \n",
              "1  журналюхи развели проф. Бабуина на разговор о ...   \n",
              "2   Возможности карьеры 1991 Прям про двачера для...   \n",
              "3             В Византии не было университетов, мань   \n",
              "4  я думал на земле нет таких людей которые смогл...   \n",
              "\n",
              "                                  Реплика 2 original  \\\n",
              "0            Слышишь блядь заткни свой ебальник сука   \n",
              "1   я высказал предположение что ... но естествен...   \n",
              "2  БЛЯДЬ СУКА ПИЗДЕЦ ВОТ ПОВЕЗЛО ЕБАНЫЙ РОТ ЗА ЧТ...   \n",
              "3  В Византии были государственные школы. До Юсти...   \n",
              "4  ЛОЛ мы чуть 4-5 не проебали СУКА Я УБИЛ КУРЫ У...   \n",
              "\n",
              "                                     Реплика 2 lemma Выделенные слова  \\\n",
              "0  ['слышать', 'блядь', 'заткнуть', 'свой', 'ебал...      блядь, сука   \n",
              "1  ['я', 'высказать', 'предположение', 'что', '.....      блядь, сука   \n",
              "2  ['блядь', 'сук', 'пиздец', 'вот', 'повезти', '...      блядь, сука   \n",
              "3  ['в', 'византия', 'быть', 'государственный', '...      блядь, сука   \n",
              "4  ['лола', 'мы', 'чуть', '-', 'не', 'проебали', ...      блядь, сука   \n",
              "\n",
              "  target_group                                      ipm  \\\n",
              "0        women  [3642.207530790995, 1712.2843529757424]   \n",
              "1        women  [3642.207530790995, 1712.2843529757424]   \n",
              "2        women  [3642.207530790995, 1712.2843529757424]   \n",
              "3        women  [3642.207530790995, 1712.2843529757424]   \n",
              "4        women  [3642.207530790995, 1712.2843529757424]   \n",
              "\n",
              "                           zipf  \\\n",
              "0  [1507.0, 430.57142857142856]   \n",
              "1  [1507.0, 430.57142857142856]   \n",
              "2  [1507.0, 430.57142857142856]   \n",
              "3  [1507.0, 430.57142857142856]   \n",
              "4  [1507.0, 430.57142857142856]   \n",
              "\n",
              "                       Реплика 2 original dialogue 1  \\\n",
              "0                               Это ты сейчас о чём?   \n",
              "1  Такое случается и в жизни 🙁Лучше поменьше чита...   \n",
              "2  А что случилось с твоими возможностями в 1991 ...   \n",
              "3                         У них были только академии   \n",
              "4  Согласна, есть много странных и даже опасных в...   \n",
              "\n",
              "                       Реплика 2 original dialogue 2 cosins_non_toxics_1  \\\n",
              "0  Вот они и хотят быть равноправными участниками...    tensor([0.4488])   \n",
              "1  Мне кажется, тут неуместно говорить «педераст»...    tensor([0.4352])   \n",
              "2                     А сейчас у тебя какая карьера?    tensor([0.3511])   \n",
              "3             Как же так? Там были философские школы    tensor([0.4177])   \n",
              "4  Согласна. Я слышала, что даже среди вегетариан...    tensor([0.6395])   \n",
              "\n",
              "       who_wins_1    cosins_origins  \n",
              "0  toxic_original  tensor([0.4709])  \n",
              "1  toxic_original  tensor([0.7187])  \n",
              "2  toxic_original  tensor([0.4075])  \n",
              "3  toxic_original  tensor([0.5465])  \n",
              "4   non_toxic_ipa  tensor([0.4455])  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTJaHM5Otz4e"
      },
      "outputs": [],
      "source": [
        "# cosins_origins = []\n",
        "# cosins_toxics = []\n",
        "# who_wins = []\n",
        "# for text_1, text_2, text_3 in tqdm(zip(dialog_1, dialog_2_origin, dialog_2_toxic)):\n",
        "#     cosin_origin, cosin_toxic, who_won = find_cosin_similarity(text_1, text_2, text_3)\n",
        "#     cosins_origins.append(cosin_origin)\n",
        "#     cosins_toxics.append(cosin_toxic)\n",
        "#     who_wins.append(who_won)\n",
        "\n",
        "# df.cosins_origins = cosins_origins\n",
        "# df.cosins_toxics = cosins_toxics\n",
        "# df.who_wins = who_wins"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "rubert",
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
