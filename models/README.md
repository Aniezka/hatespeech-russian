# Testing the Models
- rubert.ipynb - training of [RuBERT](https://huggingface.co/DeepPavlov/rubert-base-cased) (Russian, cased, 12 - layer, 768- hidden, 12- heads, 180M parameters was trained on the Russian part of Wikipedia and news data)
- rubert_model_ev.ipynb - classification report for RuBert
- rubert_xl.py - [Conversational RuBERT7](https://huggingface.co/DeepPavlov/rubert-base-cased-conversational) (Russian, cased, 12- layer, 768- hidden, 12 - heads, 180M parameters was trained on OpenSubtitles, Dirty, Pikabu, and a Social Media segment of Taiga corpus)
- gpt_3.ipynb - [RuGPT3 Large](https://huggingface.co/sberbank-ai/rugpt3large_based_on_gpt2) 
- GPT3NEW.ipynb - [RuGPT3 XL](https://huggingface.co/sberbank-ai/rugpt3xl)

Link to data: https://disk.yandex.ru/d/-tCsNY__X1yyvA 
